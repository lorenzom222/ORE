# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#
import math
import warnings
import argparse
from logging import getLogger
import pickle
import os

import numpy as np
import torch

from .logger import create_logger, PD_Stats
from functools import partial

import torch.distributed as dist
from sklearn.preprocessing import label_binarize
from sklearn.metrics._ranking import precision_recall_curve
# from sklearn.metrics._base import _average_binary_score --> defined custom
from sklearn.utils import check_array, check_consistent_length

FALSY_STRINGS = {"off", "false", "0"}
TRUTHY_STRINGS = {"on", "true", "1"}


logger = getLogger()


def _make_divisible(v, divisor, min_value=None):
    """
    This function is taken from the original tf repo.
    It ensures that all layers have a channel number that is divisible by 8
    It can be seen here:
    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
    :param v:
    :param divisor:
    :param min_value:
    :return:
    """
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    # Make sure that round down does not go down by more than 10%.
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v


# def _trunc_normal_(tensor, mean, std, a, b):
#     # Cut & paste from PyTorch official master until it's in a few official releases - RW
#     # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
#     def norm_cdf(x):
#         # Computes standard normal cumulative distribution function
#         return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

#     if (mean < a - 2 * std) or (mean > b + 2 * std):
#         warnings.warn(
#             "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
#             "The distribution of values may be incorrect.",
#             stacklevel=2,
#         )

#     # Values are generated by using a truncated uniform distribution and
#     # then using the inverse CDF for the normal distribution.
#     # Get upper and lower cdf values
#     l = norm_cdf((a - mean) / std)
#     u = norm_cdf((b - mean) / std)

#     # Uniformly fill tensor with values from [l, u], then translate to
#     # [2l-1, 2u-1].
#     tensor.uniform_(2 * l - 1, 2 * u - 1)

#     # Use inverse cdf transform for normal distribution to get truncated
#     # standard normal
#     tensor.erfinv_()

#     # Transform to proper mean, std
#     tensor.mul_(std * math.sqrt(2.0))
#     tensor.add_(mean)

#     # Clamp to ensure it's in the proper range
#     tensor.clamp_(min=a, max=b)
#     return tensor


def trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
            "The distribution of values may be incorrect.",
            stacklevel=2,
        )

    # Values are generated by using a truncated uniform distribution and
    # then using the inverse CDF for the normal distribution.
    # Get upper and lower cdf values
    l = norm_cdf((a - mean) / std)
    u = norm_cdf((b - mean) / std)

    # Uniformly fill tensor with values from [l, u], then translate to
    # [2l-1, 2u-1].
    tensor.uniform_(2 * l - 1, 2 * u - 1)

    # Use inverse cdf transform for normal distribution to get truncated
    # standard normal
    tensor.erfinv_()

    # Transform to proper mean, std
    tensor.mul_(std * math.sqrt(2.0))
    tensor.add_(mean)

    # Clamp to ensure it's in the proper range
    tensor.clamp_(min=a, max=b)
    return tensor


def bool_flag(s):
    """
    Parse boolean arguments from the command line.
    """
    if s.lower() in FALSY_STRINGS:
        return False
    elif s.lower() in TRUTHY_STRINGS:
        return True
    else:
        raise argparse.ArgumentTypeError("invalid value for a boolean flag")


def init_distributed_mode(args):
    """
    Initialize the following variables:
        - world_size
        - rank
    """

    args.is_slurm_job = "SLURM_JOB_ID" in os.environ

    if args.is_slurm_job:
        args.rank = int(os.environ["SLURM_PROCID"])
        args.world_size = int(os.environ["SLURM_NNODES"]) * int(
            os.environ["SLURM_TASKS_PER_NODE"][0]
        )
    else:
        # print(f"RANK: {args.rank}")
        # print(f"World SIZE: {args.world_size}")
        # multi-GPU job (local or multi-node) - jobs started with torch.distributed.launch
        # read environment variables
        # print(f"os.environ: {os.environ}")
        # args.rank = int(os.environ["RANK"])
        # args.world_size = int(os.environ["WORLD_SIZE"])
        args.rank = int(os.environ.get("RANK", 1))  # Set to 1 if not found
        args.world_size = int(os.environ.get("WORLD_SIZE", 1))

    # prepare distributed
    dist.init_process_group(
        backend="nccl",
        init_method=args.dist_url,
        world_size=args.world_size,
        rank=args.rank,
    )

    # set cuda device
    args.gpu_to_work_on = args.rank % torch.cuda.device_count()
    torch.cuda.set_device(args.gpu_to_work_on)
    return


def initialize_exp(params, *args, dump_params=True):
    """
    Initialize the experience:
    - dump parameters
    - create checkpoint repo
    - create a logger
    - create a panda object to keep track of the training statistics
    """

    # dump parameters
    if dump_params:
        pickle.dump(params, open(os.path.join(params.dump_path, "params.pkl"), "wb"))

    # create repo to store checkpoints
    params.dump_checkpoints = os.path.join(params.dump_path, "checkpoints")
    if not params.rank and not os.path.isdir(params.dump_checkpoints):
        os.mkdir(params.dump_checkpoints)

    # create a panda object to log loss and acc
    training_stats = PD_Stats(
        os.path.join(params.dump_path, "stats" + str(params.rank) + ".pkl"), args
    )

    # create a logger
    logger = create_logger(
        os.path.join(params.dump_path, "train.log"), rank=params.rank
    )
    logger.info("============ Initialized logger ============")
    logger.info(
        "\n".join("%s: %s" % (k, str(v)) for k, v in sorted(dict(vars(params)).items()))
    )
    logger.info("The experiment will be stored in %s\n" % params.dump_path)
    logger.info("")
    return logger, training_stats

    # logger, training_stats = initialize_exp(
    #     args, "epoch", "loss", "prec1", "prec5", "loss_val", "prec1_val", "prec5_val"
    # )


def restart_from_checkpoint(ckp_paths, run_variables=None, **kwargs):
    """
    Re-start from checkpoint
    """
    # look for a checkpoint in exp repository
    if isinstance(ckp_paths, list):
        for ckp_path in ckp_paths:
            if os.path.isfile(ckp_path):
                break
    else:
        ckp_path = ckp_paths

    if not os.path.isfile(ckp_path):
        return

    logger.info("Found checkpoint at {}".format(ckp_path))

    # open checkpoint file
    checkpoint = torch.load(
        ckp_path,
        map_location="cuda:"
        + str(torch.distributed.get_rank() % torch.cuda.device_count()),
    )

    # key is what to look for in the checkpoint file
    # value is the object to load
    # example: {'state_dict': model}
    for key, value in kwargs.items():
        if key in checkpoint and value is not None:
            try:
                msg = value.load_state_dict(checkpoint[key], strict=False)
                print(msg)
            except TypeError:
                msg = value.load_state_dict(checkpoint[key])
            logger.info("=> loaded {} from checkpoint '{}'".format(key, ckp_path))
        else:
            logger.warning(
                "=> failed to load {} from checkpoint '{}'".format(key, ckp_path)
            )

    # re load variable important for the run
    if run_variables is not None:
        for var_name in run_variables:
            if var_name in checkpoint:
                run_variables[var_name] = checkpoint[var_name]


def fix_random_seeds(seed=31):
    """
    Fix random seeds.
    """
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)


class AverageMeter(object):
    """computes and stores the average and current value"""

    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count


# def accuracy(output, target, topk=(1,)):
#     """Computes the accuracy over the k top predictions for the specified values of k"""
#     with torch.no_grad():
#         maxk = max(topk)
#         batch_size = target.size(0)

#         _, pred = output.topk(maxk, 1, True, True)
#         pred = pred.t()
#         correct = pred.eq(target.view(1, -1).expand_as(pred))

#         res = []
#         for k in topk:
#             correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)
#             res.append(correct_k.mul_(100.0 / batch_size))
#         return res


def initialize_group_dicts(label_to_group_mapping):
    """Initialize dictionaries to store correct predictions and total samples for each race."""
    correct_by_group = {}
    total_by_group = {}
    for group in label_to_group_mapping.values():
        correct_by_group[group] = 0
        total_by_group[group] = 0
    return correct_by_group, total_by_group


def print_tensor_as_grid(tensor, title):
    print(f"{title}:")
    print(np.array(tensor.cpu()))


def update_group_dicts(
    correct, target, label_to_group_mapping, correct_by_group, total_by_group
):
    """
    Update dictionaries with data from the current batch.

    Args:
    - correct: A tensor indicating whether predictions were correct for the current batch.
    - target: A tensor containing true labels for the current batch.
    - label_to_group_mapping: A dictionary mapping labels to race categories.
    - correct_by_group: A dictionary to store the count of correct predictions for each race.
    - total_by_group: A dictionary to store the total count of samples for each race.
    """
    # print(f"target: {target}")
    # print(f"correct: {correct}")
    # print(f"target.size(0): {target.size(0)}")
    # print(f"correct.size(0): {correct.size(0)}")
    num_samples = target.size(0)

    # Print the tensors as grids before the comparison
    # print_tensor_as_grid(correct, "Correct")
    # print_tensor_as_grid(target, "Target")

    for sample_index in range(num_samples):
        true_label = target[sample_index].item()
        # print(f"true_label: {true_label}")

        race = label_to_group_mapping[true_label]
        # print(label_to_group_mapping)
        # print(f"race: {race}")

        # Increment the total count for this race
        total_by_group[race] += 1
        # print(f"total_by_group: {total_by_group}")

        # print(f"After processing sample {sample_index}, total count for {race} is now {total_by_group[race]}")
        # print(f"correct: {correct}")

        pred_label = correct[sample_index].item()
        # print(f"pred_label: {pred_label}")

        # If this label was predicted correctly, increment the count for this race
        correct_by_group[race] += pred_label
        # print(f"Correct prediction for {race}, correct count is now {correct_by_group[race]}")

        # print("----")  # Separating lines for better visualization


def calculate_accuracy_by_race(correct_by_group, total_by_group):
    """
    Calculate accuracy for each race.

    Args:
    - correct_by_group: A dictionary storing the count of correct predictions for each race.
    - total_by_group: A dictionary storing the total count of samples for each race.

    Returns:
    - accuracy_by_race: A dictionary with accuracy values for each race.
    """
    accuracy_by_race = {}
    race_keys = correct_by_group.keys()
    correct_values = correct_by_group.values()
    total_values = total_by_group.values()

    zipped_data = list(zip(race_keys, correct_values, total_values))

    for race, correct, total in zipped_data:
        if total > 0:
            accuracy = correct * (100.0 / total)
            accuracy_by_race[race] = accuracy
        else:
            accuracy_by_race[race] = 0

    return accuracy_by_race


def accuracy(output, target, topk=(1,), label_to_group_mapping=None):
    """Computes the accuracy over the k top predictions for the specified values of k, and records accuracy by race"""
    with torch.no_grad():
        maxk = max(topk)
        batch_size = target.size(0)

        maxk = max(topk)
        batch_size = target.size(0)

        _, pred = output.topk(maxk, 1, True, True)
        pred = pred.t()

        target_viewed = target.view(1, -1)
        target_expand_pred = target_viewed.expand_as(pred)

        correct = pred.eq(target_expand_pred)


        res = []
        acc_by_race_topk = []

        for k in topk:
            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)
            average_overall = correct_k.mul_(100.0 / batch_size)
            res.append(average_overall)

        if label_to_group_mapping:
            for k in topk:
                target_expand_k = target_expand_pred[:k].reshape(-1)
                correct_k = correct[:k].reshape(-1).float()
                correct_by_group, total_by_group = initialize_group_dicts(
                    label_to_group_mapping
                )
                update_group_dicts(
                    correct_k,
                    target_expand_k,
                    label_to_group_mapping,
                    correct_by_group,
                    total_by_group,
                )
                accuracy_by_race = calculate_accuracy_by_race(
                    correct_by_group, total_by_group
                )
                acc_by_race_topk.append(accuracy_by_race)
            # print("res: ", res)
            return res, acc_by_race_topk

        return res

import random
def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)


def average_precision_score(
    y_true, y_score, *, average="macro", pos_label=1, labels_to_races=None, sample_weight=None
):
    def _binary_uninterpolated_average_precision(
            y_true, y_score, pos_label=1, sample_weight=None
        ):
            precision, recall, _ = precision_recall_curve(
                y_true, y_score, pos_label=pos_label, sample_weight=sample_weight
            )
            # Return the step function integral
            # The following works because the last entry of precision is
            # guaranteed to be 1, as returned by precision_recall_curve
            return -np.sum(np.diff(recall) * np.array(precision)[:-1])
    
    #multi class only, modified from sklearn source code

    # Convert to Python primitive type to avoid NumPy type / Python str
    # comparison. See https://github.com/numpy/numpy/issues/6784
    present_labels = np.unique(y_true).tolist()

    if pos_label != 1:
        raise ValueError(
            "Parameter pos_label is fixed to 1 for multiclass y_true. "
            "Do not set pos_label or set pos_label to 1."
        )
    y_true = label_binarize(y_true, classes=present_labels)

    average_precision = partial(
        _binary_uninterpolated_average_precision, pos_label=pos_label
    )

    return _average_binary_score(
        average_precision, y_true, y_score, average, labels_to_races, sample_weight=sample_weight
    )


def _average_binary_score(binary_metric, y_true, y_score, average, labels_to_groups,
                          sample_weight=None):
    
    group_ap_dict = {group: [0,0] for group in labels_to_groups.values()}

    def update_group_score(class_pos, class_name, score_c):
        group_label = labels_to_groups[class_name]
        group = group_label
        num_class_c = np.count_nonzero(y_true[:, class_pos])
        group_ap_dict[group][0] += score_c
        group_ap_dict[group][1] += num_class_c

    check_consistent_length(y_true, y_score, sample_weight)
    y_true = check_array(y_true)
    y_score = check_array(y_score)

    not_average_axis = 1
    score_weight = sample_weight
    average_weight = None

    if y_true.ndim == 1:
        y_true = y_true.reshape((-1, 1))

    if y_score.ndim == 1:
        y_score = y_score.reshape((-1, 1))

    # n_classes = y_score.shape[not_average_axis]
    n_classes = y_true.shape[not_average_axis]
    # get the first col that has non-zero values
    cur_min_class = list(labels_to_groups.keys())[0]
    score = np.zeros((n_classes,))
    print("n_classes: ", n_classes)
    print("cur_min_class: ", cur_min_class)

    for c in range(n_classes):
        cur_class = c
        y_true_c = y_true.take([c], axis=not_average_axis).ravel()
        y_score_c = y_score.take([cur_class], axis=not_average_axis).ravel()
        score[c] = binary_metric(y_true_c, y_score_c, sample_weight=score_weight)
        update_group_score(c, cur_class, score[c])

    #TODO: add as param
    num_class_per_race = 10

    for group, val in group_ap_dict.items():
        if val[1] > 0:
            val[0] /= num_class_per_race
    print("total mean ap: ", np.average(score, weights=average_weight))
    return group_ap_dict
